{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpBTUxqw3fstuTkWa9Vivw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hongdnn/unsloth_finetuning_demo/blob/main/UnslothFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7kd2910PvfH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "  !pip install unsloth\n",
        "else:\n",
        "  # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "  !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "  !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "  !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name= \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length= max_seq_length,\n",
        "    dtype= dtype,\n",
        "    load_in_4bit= load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "i--CS8wOS5RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "LZLjP-VOWxMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,  # The base pre-trained model that you want to fine-tune using PEFT (Parameter-Efficient Fine-Tuning) techniques.\n",
        "\n",
        "    # r: The LoRA rank, which determines the number of trainable parameters in the low-rank adapters.\n",
        "    # A higher value (e.g., 128) gives the adapter more capacity to learn task-specific nuances,\n",
        "    # while lower values (e.g., 8, 16, 32, 64) might be sufficient for simpler tasks.\n",
        "    r = 128,  # Choose any number > 0. Suggested values are 8, 16, 32, 64, 128.\n",
        "\n",
        "    # target_modules: A list of module names in the model where the LoRA adapters should be applied.\n",
        "    # For transformer-based models, these typically include:\n",
        "    # - \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\": The projection layers in the multi-head attention mechanism.\n",
        "    # - \"gate_proj\", \"up_proj\", \"down_proj\": Additional projection layers used in various architectures or gating mechanisms.\n",
        "    # - \"embed_tokens\", \"lm_head\": Typically included for continual pretraining or when modifying the embedding and output layers.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                      \"embed_tokens\", \"lm_head\"],  # Add these for continual pretraining if needed.\n",
        "\n",
        "    # lora_alpha: A scaling factor applied to the LoRA weights.\n",
        "    # This factor adjusts the influence of the LoRA adapters relative to the original model weights.\n",
        "    lora_alpha = 32,\n",
        "\n",
        "    # lora_dropout: Dropout rate for the LoRA adapters.\n",
        "    # A dropout of 0 means no dropout is applied, which is the optimized setting in this configuration.\n",
        "    lora_dropout = 0,  # Supports any value, but 0 is optimized.\n",
        "\n",
        "    # bias: Configures the use of bias parameters in the adapters.\n",
        "    # Setting this to \"none\" means no additional bias terms are introduced, which simplifies the model.\n",
        "    bias = \"none\",  # Supports any value, but \"none\" is optimized.\n",
        "\n",
        "    # use_gradient_checkpointing: A technique to reduce memory usage by trading compute for memory.\n",
        "    # The special \"unsloth\" setting here is an optimized mode that reportedly reduces VRAM usage by 30%\n",
        "    # and allows for twice as large batch sizes, which is particularly useful for very long context lengths.\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Use True or \"unsloth\" for very long context scenarios.\n",
        "\n",
        "    # random_state: Sets a seed for the random number generator to ensure reproducibility in training.\n",
        "    random_state = 3407,\n",
        "\n",
        "    # use_rslora: Activates Rank Stabilized LoRA (RS-LoRA), an enhanced version designed to improve stability during training.\n",
        "    use_rslora = True,  # Enables the use of rank stabilized LoRA.\n",
        "\n",
        "    # loftq_config: Configuration for LoftQ, another parameter-efficient fine-tuning technique.\n",
        "    # Here it is set to None, meaning that LoftQ is not applied in this configuration.\n",
        "    loftq_config = None,  # LoftQ is not used in this case.\n",
        ")\n"
      ],
      "metadata": {
        "id": "z3nY8m6sVDVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in case load_dataset cause error\n",
        "#pip install -U datasets"
      ],
      "metadata": {
        "id": "1nfwBdAuaxBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "squad = load_dataset(\"rajpurkar/squad\", split=\"train[:2000]\")\n",
        "squad = squad.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "zKOdOWZwYOmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = squad['train'][0]\n",
        "text"
      ],
      "metadata": {
        "id": "2JsslXkQa9Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Preprocess\n",
        "def preprocess_function(examples):\n",
        "    contexts = examples[\"context\"]\n",
        "    questions = examples[\"question\"]\n",
        "    answers = [ans[\"text\"][0] for ans in examples[\"answers\"]]\n",
        "    inputs = [f\"Context: {context} Question: {question}\" for context, question in zip(contexts, questions)]\n",
        "    return {\"input_text\": inputs, \"target_text\": answers}\n",
        "\n",
        "squad_processed = squad.map(preprocess_function, batched=True, num_proc=8, remove_columns=[\"id\", \"title\", \"context\", \"question\", \"answers\"])\n",
        "\n",
        "# Formatting with stop token\n",
        "def formatting_func(example):\n",
        "    return f\"{example['input_text']} Answer: {example['target_text']}<|eot|>\""
      ],
      "metadata": {
        "id": "sAJ-O3kLb3UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_processed[\"train\"][0]"
      ],
      "metadata": {
        "id": "JAqyRsvsb9L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatting_func(squad_processed[\"train\"][1])"
      ],
      "metadata": {
        "id": "UxxQzHuAcALj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        ""
      ],
      "metadata": {
        "id": "WH6BYFpgdjjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) #not using mask language\n",
        "\n",
        "# Trainer\n",
        "trainer = UnslothTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=squad_processed[\"train\"],\n",
        "    dataset_text_field=\"input_text\",\n",
        "    formatting_func=formatting_func,\n",
        "    max_seq_length=512,\n",
        "    data_collator=data_collator,\n",
        "    args=UnslothTrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        warmup_ratio=0.1,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=5e-5,\n",
        "        embedding_learning_rate=5e-6,\n",
        "        fp16 = not is_bfloat16_supported(), # Use 16-bit floating point if bfloat16 isn't supported.\n",
        "        bf16 = is_bfloat16_supported(),     #use bf16 if hardware support\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.00,\n",
        "        lr_scheduler_type=\"cosine\", #Cosine scheduler to adjust the learning rate over time.\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1M3cNH8Pdm0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "text_streamer = TextIteratorStreamer(tokenizer)\n",
        "import textwrap\n",
        "max_print_width = 100\n",
        "\n",
        "# Before running inference, call `FastLanguageModel.for_inference` first\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    \"<|begin_of_text|>Context: Albert Einstein was a theoretical physicist known for his work on relativity. \"\n",
        "    \"His contributions revolutionized the understanding of space, time, and gravity. \"\n",
        "    \"Question: What is the theory that made Albert Einstein famous? \"\n",
        "    \"Answer:\"\n",
        "\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "generation_kwargs = dict(\n",
        "    inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 25,\n",
        "    use_cache = True,\n",
        ")\n",
        "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "thread.start()\n",
        "\n",
        "# Accumulate the streamed text.\n",
        "generated_text = \"\"\n",
        "for new_text in text_streamer:\n",
        "    generated_text += new_text\n",
        "\n",
        "# Post-process the generated text to extract only the answer.\n",
        "# The answer should starts after \"Answer:\"\n",
        "if \"Answer:\" in generated_text:\n",
        "    answer_section = generated_text.split(\"Answer:\", 1)[1]\n",
        "    # If a new question appears, stop there.\n",
        "    if \"Question:\" in answer_section:\n",
        "        answer = answer_section.split(\"Question:\")[0].strip()\n",
        "    else:\n",
        "        answer = answer_section.strip()\n",
        "else:\n",
        "    answer = generated_text.strip()\n",
        "\n",
        "# Optionally, wrap the text for display\n",
        "wrapped_answer = \"\\n\".join(textwrap.wrap(answer, width=max_print_width))\n",
        "print(wrapped_answer)"
      ],
      "metadata": {
        "id": "RQ2m-rbFe-Te"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}